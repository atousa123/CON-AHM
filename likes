install.packages("academictwitteR")
library(academictwitteR)
install.packages("jsonlite")
library(jsonlite)
install.packages("rjson")
library(rjson)
install.packages("httr")
library(httr)
install.packages("ggplot2")
library(ggplot2)
install.packages("plyr")
library(plyr)
install.packages("dplyr")
library(dplyr)
install.packages("stringr")
library(stringr)
install.packages("rtweet")
library(rtweet)
install.packages("twitteR")
library(twitteR)
install.packages("magrittr")
library(magrittr)
install.packages("lubridate")
library(lubridate)
install.packages("arules")
install.packages("arulesViz")
install.packages("tidyverse")
install.packages("purrr")

# Loading package
library(arules)
library(arulesViz)
library(tidyverse)
library(purrr)
library(readxl)
library(knitr)

install.packages("tm")
install.packages("igraph")
install.packages("ggplot2")

library(tm)
library(igraph)
library(ggplot2)
install.packages("tidytext")

library(tidytext)

install.packages("stringr")
library(stringr)

# bind json files in the directory "data" into a data frame containing tweets
bind_tweets(data_path = "C:\\Users\\atoos\\OneDrive\\Desktop\\American Heart Month data json")
# bind json files in the directory "data" into a data frame containing user information
bind_tweets(data_path = "C:\\Users\\atoos\\OneDrive\\Desktop\\American Heart Month data json", user = TRUE)
# bind json files in the directory "data" into a "tidy" data frame / tibble
tweets_AmericanHeartMonth <- bind_tweets(data_path = "C:\\Users\\atoos\\OneDrive\\Desktop\\American Heart Month data json", user = TRUE, output_format = "tidy")


AHM_likes <- tweets_AmericanHeartMonth %>% select (text, created_at, lang, like_count) %>% filter (lang %in% c("en"), tweets_AmericanHeartMonth$like_count > 0) %>% na.omit() 

##create pattern to search hashtags within text
hashtag_pat <- "#[a-zA-Z0-9_-ー\\.]+"
hashtag_likes <- str_extract_all(AHM_likes$text, hashtag_pat)
hashtag_word_likes <- unlist(hashtag_likes)
hashtag_word_likes <- tolower(hashtag_word_likes)
hashtag_word_likes <- gsub("[[:punct:]ー]", "", hashtag_word_likes)
hashtag_word_likes <- hashtag_word_likes[!str_detect(hashtag_word_likes, "AmericanHeartMonth")]

glimpse(hashtag_word_likes)

hashtag_count_likes <- table(hashtag_word_likes)
top_10_hashtag_likes <- sort(hashtag_count_likes, decreasing = TRUE)[1:10]
top_10_hashtag_likes

as.data.frame(hashtag_word_likes) %>%
  count(hashtag_word_likes, sort = TRUE) %>%
  mutate(hashtag_word_likes = reorder(hashtag_word_likes, n)) %>%
  top_n(10) %>%
  ggplot(aes(x = hashtag_word_likes, y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Count",
       y = "Hashtag_likes",
       title = "Top 10 hashtags used in likes with #AmericanHeartMonth")

corpus <- Corpus(VectorSource(hashtag_likes))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
cooc_matrix <- crossprod(as.matrix(dtm))

# Create an adjacency matrix
adjacency_matrix <- as.matrix(cooc_matrix)

# Create an igraph graph
g3 <- graph.adjacency(adjacency_matrix, weighted = TRUE, mode = "undirected")

# You can also filter the network to remove low-weight edges to reduce noise
# Set your desired threshold value
threshold_value <- 0.5 # Adjust this value as needed

# Filter the network to remove low-weight edges
g3 <- delete_edges(g3, E(g3)[weight < threshold_value])

# Remove nodes with degrees below a certain threshold (e.g., degree < 5)
g3<- delete_vertices(g3, V(g3)[degree(g3) <  454 ])

# Plot the network
plot(g3, layout = layout.fruchterman.reingold, edge.arrow.size = 0.1)


saveRDS(hashtag_likes, file = "hashtag_likes.rds")
saveRDS(hashtag_count_likes, file = "hashtag_count_likes.rds")
saveRDS(hashtag_word_likes, file = "hashtag_word_likes.rds")

install.packages("igraph")
library(igraph)
# Create a graph object from your data
con_graph <- graph_from_data_frame(hashtag_count_likes, directed = FALSE)
num_nodes <- vcount(con_graph)

num_edges <- ecount(con_graph)

cat("Number of nodes (hashtags):", num_nodes, "\n")
cat("Number of edges (co-occurrences):", num_edges, "\n")

con_density <- graph.density(con_graph)
cat("Density of the Co-Occurrence Network:", con_density, "\n")
